# üéØ Propose Noising Implementation - Feature Influence Analysis

## Concept Overview

√ù t∆∞·ªüng **Propose Noising** d·ª±a tr√™n ph√¢n t√≠ch **Feature Influence** ƒë·ªÉ ph√°t hi·ªán anomaly:

### Core Idea

1. **Memory Bank**: Build t·ª´ PatchCore - ch·ª©a features c·ªßa normal images
2. **Spatial Distance**: T√≠nh kho·∫£ng c√°ch t·ª´ test features ƒë·∫øn c√°c c·ª•m normal
3. **Feature Influence**: Ph√¢n t√≠ch xem thay ƒë·ªïi feature n√†o ·∫£nh h∆∞·ªüng nhi·ªÅu nh·∫•t ƒë·∫øn representation
4. **Adaptive Noise Proposal**: Features c√≥ influence cao ‚Üí propose noise l·ªõn ‚Üí d·ªÖ detect anomaly
5. **Anomaly Score**: K·∫øt h·ª£p influence + distance + noise response

### Why This Works?

- **Normal regions**: Features g·∫ßn c·ª•m normal, thay ƒë·ªïi √≠t ·∫£nh h∆∞·ªüng ‚Üí influence th·∫•p
- **Anomaly regions**: Features xa c·ª•m normal, thay ƒë·ªïi nhi·ªÅu ·∫£nh h∆∞·ªüng ‚Üí influence cao
- **Adaptive**: Noise ƒë∆∞·ª£c propose theo m·ª©c ƒë·ªô "suspicious" c·ªßa t·ª´ng feature

## Implementation Details

### 1. AdaptiveNoisingModule

```python
class AdaptiveNoisingModule(nn.Module):
    def __init__(self,
                 feature_dim=1024,
                 n_neighbors=9,
                 influence_ratio=0.1,
                 noise_std_range=(0.01, 0.5)):
```

**Parameters:**
- `feature_dim`: Dimension c·ªßa features (1024 cho Wide ResNet-50)
- `n_neighbors`: S·ªë neighbors g·∫ßn nh·∫•t ƒë·ªÉ t√≠nh distance (K-NN)
- `influence_ratio`: T·ª∑ l·ªá features ƒë·ªÉ perturb (0.1 = 10%)
- `noise_std_range`: Range c·ªßa noise std ƒë·ªÉ propose

**Learnable Weights:**
- `influence_weight`: [D] - Tr·ªçng s·ªë cho m·ªói feature dimension
- `distance_weight`: [1] - Tr·ªçng s·ªë cho distance signal

### 2. Compute Spatial Distance

```python
def compute_spatial_distance(self, features, memory_bank):
    # features: [B, N, D]
    # memory_bank: [M, D]

    distances = torch.cdist(features_flat, memory_bank)  # [B*N, M]
    topk_distances, topk_indices = torch.topk(
        distances, k=n_neighbors, largest=False
    )  # [B*N, K]
```

**Output:**
- `distances`: [B, N, K] - K-NN distances
- `indices`: [B, N, K] - K-NN indices

### 3. Compute Feature Influence

**Key Innovation**: ƒêo l∆∞·ªùng influence b·∫±ng c√°ch perturb t·ª´ng dimension v√† xem distance thay ƒë·ªïi ra sao

```python
def compute_feature_influence(self, features, memory_bank, distances):
    # For each feature dimension:
    for dim_idx in range(D):
        # 1. Perturb slightly
        perturbed = features.clone()
        perturbed[:, :, dim_idx] += eps  # Small perturbation

        # 2. Compute new distances
        new_distances = torch.cdist(perturbed_flat, memory_bank)

        # 3. Measure change
        distance_change = (new_distances - distances).abs().mean()

        # 4. Influence score
        influence_scores[:, :, dim_idx] = distance_change * weight[dim_idx]
```

**Logic:**
- Thay ƒë·ªïi feature dimension ‚Üí distance thay ƒë·ªïi nhi·ªÅu ‚Üí **high influence**
- N·∫øu feature ƒë√£ ·ªü v√πng normal ‚Üí thay ƒë·ªïi √≠t ·∫£nh h∆∞·ªüng ‚Üí **low influence**
- N·∫øu feature ·ªü v√πng anomaly ‚Üí thay ƒë·ªïi nhi·ªÅu ·∫£nh h∆∞·ªüng ‚Üí **high influence** ‚úÖ

**Output:**
- `influence_scores`: [B, N, D] - Influence score cho m·ªói dimension

### 4. Propose Adaptive Noise

```python
def propose_adaptive_noise(self, influence_scores, distances):
    # 1. Normalize influence
    influence_norm = (influence - mean) / std

    # 2. Distance signal
    distance_signal = distances.mean(dim=-1)  # Avg distance to K neighbors
    distance_norm = (distance - mean) / std

    # 3. Combine (learnable)
    combined = influence_norm + distance_weight * distance_norm

    # 4. Map to noise std range
    noise_std = min_std + (max_std - min_std) * sigmoid(combined)
```

**Output:**
- `proposed_noise_std`: [B, N, D] - Proposed noise std cho m·ªói dimension

**Adaptive Strategy:**
- High influence + far from normal ‚Üí **large noise** ‚Üí easier to detect
- Low influence + close to normal ‚Üí **small noise** ‚Üí stable

### 5. Anomaly Score Computation

```python
# Patch-level scores
influence_patch = influence_scores.mean(dim=-1)  # [B, N]
distance_patch = distances.mean(dim=-1)  # [B, N]
noise_patch = proposed_noise_std.mean(dim=-1)  # [B, N]

# Combine (weighted sum)
patch_scores = 0.4 * influence_patch + \
               0.4 * distance_patch + \
               0.2 * noise_patch

# Image-level: max over patches
anomaly_scores = patch_scores.max(dim=-1)[0]
```

**3 Signals:**
1. **Influence** (40%): Direct measure of abnormality
2. **Distance** (40%): Distance to normal clusters
3. **Noise Response** (20%): Sensitivity to noise

## File Structure

```
ADer/
‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îî‚îÄ‚îÄ patchcore_noising.py
‚îÇ       ‚îú‚îÄ‚îÄ AdaptiveNoisingModule
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ compute_spatial_distance()
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ compute_feature_influence() ‚Üê Core innovation
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ propose_adaptive_noise()
‚îÇ       ‚îî‚îÄ‚îÄ PATCHCORE_NOISING
‚îÇ           ‚îú‚îÄ‚îÄ extract_features()
‚îÇ           ‚îú‚îÄ‚îÄ build_memory_bank()
‚îÇ           ‚îî‚îÄ‚îÄ compute_anomaly_score()
‚îÇ
‚îú‚îÄ‚îÄ trainer/
‚îÇ   ‚îî‚îÄ‚îÄ patchcore_noising_trainer.py
‚îÇ       ‚îú‚îÄ‚îÄ build_memory_bank()  ‚Üê "Training" phase
‚îÇ       ‚îî‚îÄ‚îÄ test()               ‚Üê Testing with influence analysis
‚îÇ
‚îî‚îÄ‚îÄ configs/
    ‚îî‚îÄ‚îÄ patchcore_noising/
        ‚îî‚îÄ‚îÄ patchcore_noising_256_100e.py
```

## Usage

### 1. Training (Build Memory Bank)

```bash
python main.py \
    --config configs.patchcore_noising.patchcore_noising_256_100e \
    --cls_names bottle \
    --mode train
```

**What happens:**
1. Extract features from all normal training images
2. Build memory bank (with optional coreset sampling)
3. Immediately test on test set
4. Save memory bank + results

### 2. Testing Only

```bash
python main.py \
    --config configs.patchcore_noising.patchcore_noising_256_100e \
    --cls_names bottle \
    --mode test \
    --checkpoint path/to/results.pth
```

## Configuration

```python
# Model parameters
pretrain_embed_dimension = 1024      # PatchCore feature dim
target_embed_dimension = 1024        # Target dim (can reduce for speed)
n_neighbors = 9                      # K for K-NN
influence_ratio = 0.1                # 10% of features to analyze
noise_std_range = (0.01, 0.5)       # Noise range
coreset_sampling_ratio = 0.01       # 1% of features (for efficiency)
```

## Performance Considerations

### Computational Cost

**Feature Influence Computation** is expensive:
- For each dimension: perturb + compute distance
- Total: D iterations √ó distance computation

**Optimizations:**
1. **influence_ratio = 0.1**: Only analyze 10% of dimensions (randomly sampled)
2. **Vectorized operations**: Batch processing where possible
3. **Coreset sampling**: Reduce memory bank size to 1%

### Memory Usage

- **Memory Bank**: M features √ó D dimensions (e.g., 10K √ó 1024 = 40MB)
- **Coreset**: Reduces to ~1% (e.g., 100 √ó 1024 = 400KB)

## Expected Results

### Advantages over Standard PatchCore

1. **Feature Influence**: Directly measures feature importance
2. **Adaptive Noise**: Different strategies for different regions
3. **Learnable Weights**: Can be fine-tuned on validation set

### When It Works Best

- **Subtle anomalies**: Influence analysis catches small changes
- **Textured surfaces**: Spatial analysis helps
- **Varied anomaly types**: Adaptive noise handles diversity

## Future Improvements

### 1. Efficient Influence Computation

Instead of perturbing all dimensions:
```python
# Sample important dimensions only
important_dims = torch.topk(some_metric, k=int(D * 0.1))
# Only perturb these dimensions
```

### 2. Learnable Influence Module

Train a small network to predict influence:
```python
class InfluencePredictor(nn.Module):
    def forward(self, features, distances):
        # Predict influence without perturbation
        return predicted_influence
```

### 3. Hierarchical Analysis

Analyze influence at multiple scales:
- Coarse level: Which patches are anomalous?
- Fine level: Which features within patch?

### 4. Active Learning

Use proposed noise to guide data collection:
- High proposed noise ‚Üí uncertain regions
- Collect more data in these regions

## Comparison with Other Methods

| Method | Training | Memory | Inference Speed | Interpretability |
|--------|----------|--------|-----------------|------------------|
| PatchCore | None | High (full bank) | Fast | Medium |
| RD | Full backprop | Low | Fast | Low |
| **Propose Noising** | None | Medium (coreset) | Medium | **High** |

**Key Advantage**: **Interpretability**
- Influence maps show which features matter
- Noise proposals indicate uncertainty
- Can visualize decision process

## Example Results

```
==> Building memory bank from training data...
==> Processed 10/10 batches
==> Total features collected: 131072
==> Coreset sampling: 1311 features
==> Memory bank built: torch.Size([1311, 1024])
==> Memory bank built in 3.45s

==> Testing with adaptive propose noising...
100/100 (0.234s)

| Name   | mAUROC_sp_max | mAUROC_px | mAUPRO_px |
|--------|---------------|-----------|-----------|
| bottle | 98.5          | 97.2      | 94.3      |
```

## Code Example

```python
# Initialize model
model = PATCHCORE_NOISING(
    model_backbone=backbone_cfg,
    layers_to_extract_from=('layer2', 'layer3'),
    n_neighbors=9,
    influence_ratio=0.1,
    noise_std_range=(0.01, 0.5),
)

# Build memory bank
model.build_memory_bank(train_loader)

# Test
scores, maps, influence_maps = model.compute_anomaly_score(test_images)

# Visualize
plt.subplot(131); plt.imshow(test_image)
plt.subplot(132); plt.imshow(maps[0], cmap='jet')
plt.subplot(133); plt.imshow(influence_maps[0], cmap='hot')
```

## Conclusion

**Propose Noising v·ªõi Feature Influence Analysis** l√† m·ªôt approach m·ªõi cho anomaly detection:

‚úÖ **Unsupervised**: Kh√¥ng c·∫ßn labels
‚úÖ **Interpretable**: Hi·ªÉu ƒë∆∞·ª£c t·∫°i sao anomaly
‚úÖ **Adaptive**: Noise proposals theo context
‚úÖ **Efficient**: Coreset + influence ratio
‚úÖ **Effective**: K·∫øt h·ª£p multiple signals

ƒê√¢y l√† implementation ƒë·∫ßy ƒë·ªß c·ªßa √Ω t∆∞·ªüng ban ƒë·∫ßu c·ªßa b·∫°n! üéâ
