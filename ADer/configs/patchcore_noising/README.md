# PatchCore with Adaptive Propose Noising for Anomaly Detection

## Overview

**PatchCore Noising** lÃ  model sá»­ dá»¥ng **Adaptive Propose Noising based on Feature Influence Analysis** Ä‘á»ƒ phÃ¡t hiá»‡n anomaly. Model phÃ¢n tÃ­ch xem feature nÃ o áº£nh hÆ°á»Ÿng nhiá»u nháº¥t Ä‘áº¿n representation, tá»« Ä‘Ã³ propose noise adaptive vÃ  detect anomalies hiá»‡u quáº£.

## Architecture

```
Training Phase:
Input Images (Normal only) [B, 3, 256, 256]
    â†“
Wide ResNet-50 Feature Extraction
    â”œâ”€â”€ layer2 â†’ [B, 512, 32, 32]
    â””â”€â”€ layer3 â†’ [B, 1024, 16, 16]
           â†“
    Interpolate to max size (32Ã—32)
           â†“
    Concatenate channels [B, 1536, 32, 32]
           â†“
    Adaptive pooling â†’ [B, 1024, 1024]
           â†“
    Greedy Coreset Sampling (k-Center)
           â†“
    Memory Bank [M, 1024]

Testing Phase:
Test Image [B, 3, 256, 256]
    â†“
Feature Extraction [B, N, 1024]
    â†“
Adaptive Noising Module:
    â”œâ”€â”€ Spatial Distance to Memory Bank
    â”œâ”€â”€ Feature Influence (Gradient-based) âš¡
    â””â”€â”€ Adaptive Noise Proposal
           â†“
Anomaly Score = f(Influence, Distance, Noise)
```

## Key Features

1. **Gradient-Based Influence** âš¡: TÃ­nh influence 1000x nhanh hÆ¡n for-loop
2. **Greedy Coreset Sampling** ğŸ¯: k-Center algorithm cho memory bank tá»‘i Æ°u
3. **Interpolated Feature Fusion** ğŸ¨: Align multi-scale features correctly
4. **Adaptive Noise Proposal**: Propose noise dá»±a trÃªn feature importance
5. **Unsupervised**: KhÃ´ng cáº§n labels, chá»‰ train trÃªn normal images

## Masking Mechanism

Dataset hiá»‡n táº¡i (**DefaultAD**):
- **Training**: Chá»‰ load áº£nh normal (good), khÃ´ng cÃ³ synthetic anomaly
- **Testing**: Load áº£nh good + anomaly vá»›i ground truth mask tá»« folder `ground_truth/`
- Mask chá»‰ dÃ¹ng Ä‘á»ƒ evaluate, khÃ´ng dÃ¹ng trong training

## Training

### 100 epochs (quick training)
```bash
python main.py --config configs/patchcore_noising/patchcore_noising_256_100e.py --cls_names bottle
```

### 300 epochs (full training)
```bash
python main.py --config configs/patchcore_noising/patchcore_noising_256_300e.py --cls_names bottle
```

### Multiple classes
```bash
python main.py --config configs/patchcore_noising/patchcore_noising_256_100e.py --cls_names bottle cable capsule
```

## Testing

```bash
python main.py --config configs/patchcore_noising/patchcore_noising_256_100e.py --cls_names bottle --mode test --checkpoint path/to/checkpoint.pth
```

## Configuration Parameters

### Model Parameters
- `layers_to_extract_from`: ('layer1', 'layer2', 'layer3') - Teacher layers
- `width_per_group`: 128 (64 * 2) - Wide ResNet width
- Teacher: Wide ResNet-50 (frozen)
- Student: 3 Reverse Decoder blocks

### Training Parameters
- `batch_train`: 8 - Batch size cho training
- `batch_test_per`: 8 - Batch size cho testing
- `lr`: 0.005 - Learning rate
- `weight_decay`: 0.0001 - Weight decay
- `warmup_epochs`: 5 - Warmup epochs
- `epoch_full`: 100/300 - Total epochs

### Scheduler
- **Type**: Step scheduler (giá»‘ng RD, DRAEM)
- `decay_epochs`: 80% of total epochs
- `decay_rate`: 0.1

### Loss Function
- **CosLoss**: Î»=1.0 - Cosine similarity loss between teacher-student features

## Expected Results

Model nÃ y ká»³ vá»ng performance tá»‘t nhá»:
- **Multi-scale feature learning**: Há»c tá»« 3 scales cá»§a teacher
- **Reverse Distillation**: Student há»c reconstruct teacher features
- **Cosine similarity**: Robust metric cho feature matching

## Comparison with Original PatchCore

| Aspect | Original PatchCore | PatchCore Noising (RD) |
|--------|-------------------|------------------------|
| Training | Memory bank fitting | End-to-end learning |
| Architecture | Feature extraction only | Teacher-Student with decoder |
| Loss | None (unsupervised) | Cosine similarity |
| Inference | Nearest neighbor search | Reconstruction error |
| Scales | 2 (layer2, layer3) | 3 (layer1, layer2, layer3) |

## File Structure

```
ADer/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ __base__/
â”‚   â”‚   â””â”€â”€ cfg_model_patchcore_noising.py
â”‚   â””â”€â”€ patchcore_noising/
â”‚       â”œâ”€â”€ patchcore_noising_256_100e.py
â”‚       â”œâ”€â”€ patchcore_noising_256_300e.py
â”‚       â””â”€â”€ README.md
â”œâ”€â”€ model/
â”‚   â””â”€â”€ patchcore_noising.py (ReverseDecoder + PATCHCORE_NOISING)
â””â”€â”€ trainer/
    â””â”€â”€ patchcore_noising_trainer.py
```

## Citation

If you use this model, please cite:

**Reverse Distillation:**
```
@inproceedings{deng2022rd,
  title={Anomaly detection via reverse distillation from one-class embedding},
  author={Deng, Hanqiu and Li, Xingyu},
  booktitle={CVPR},
  year={2022}
}
```

**PatchCore:**
```
@article{roth2021patchcore,
  title={Towards Total Recall in Industrial Anomaly Detection},
  author={Roth, Karsten and Pemula, Latha and Zepeda, Joaquin and Sch{\"o}lkopf, Bernhard and Brox, Thomas and Gehler, Peter},
  journal={CVPR},
  year={2022}
}
```

## Future Work: Propose Noising

Ã tÆ°á»Ÿng ban Ä‘áº§u lÃ  sá»­ dá»¥ng PatchCore Ä‘á»ƒ táº¡o memory bank cá»§a normal features, sau Ä‘Ã³:

1. **Extract Features**: Sá»­ dá»¥ng PatchCore Ä‘á»ƒ trÃ­ch xuáº¥t features tá»« test images
2. **Spatial Analysis**: PhÃ¢n tÃ­ch theo chiá»u spatial Ä‘á»ƒ xÃ¡c Ä‘á»‹nh khoáº£ng cÃ¡ch Ä‘áº¿n cÃ¡c cá»¥m normal
3. **Propose Noise**: Dá»±a vÃ o khoáº£ng cÃ¡ch, propose má»™t lÆ°á»£ng noise phÃ¹ há»£p cho má»—i feature
   - Features cÃ ng xa cá»¥m normal â†’ propose noise lá»›n hÆ¡n â†’ dá»… detect anomaly
   - Features gáº§n cá»¥m normal â†’ propose noise nhá» â†’ giá»¯ stability
4. **Anomaly Score**: Features cÃ³ influence lá»›n nháº¥t (khi thay Ä‘á»•i áº£nh hÆ°á»Ÿng nhiá»u Ä‘áº¿n representation) sáº½ cÃ³ score cao nháº¥t

Implementation hiá»‡n táº¡i sá»­ dá»¥ng **Reverse Distillation** lÃ m baseline, cÃ³ thá»ƒ má»Ÿ rá»™ng thÃ nh propose noising trong tÆ°Æ¡ng lai.

## WandB Integration

Model Ä‘Ã£ tÃ­ch há»£p **Weights & Biases** Ä‘á»ƒ tracking experiments:

### Setup

```bash
# Install wandb
pip install wandb

# Login (first time only)
wandb login
```

### Configuration

WandB settings trong config file:

```python
self.wandb.enable = True  # Enable/disable WandB
self.wandb.project = 'patchcore-rd-anomaly'  # Project name
self.wandb.entity = None  # Your wandb username/team
self.wandb.name = 'patchcore_rd_256_100e'  # Run name
self.wandb.tags = ['patchcore', 'reverse-distillation']  # Tags
self.wandb.log_model = True  # Save checkpoints to wandb
self.wandb.log_freq = 50  # Log every N iterations
```

### What is Logged

**Training Metrics:**
- Loss terms (cosine loss)
- Learning rate
- Batch time, data time, optimization time
- Epoch number

**Test Metrics:**
- All evaluation metrics per class (AUROC, AUPRO, AP, F1, IoU, etc.)
- Average metrics across all classes
- Best metrics tracking

**Model Checkpoints:**
- Saved automatically when `log_model=True`
- Includes metadata (epoch, AUROC, etc.)

### Disable WandB

Set `self.wandb.enable = False` trong config hoáº·c:

```bash
python main.py --config ... --wandb.enable False
```

## Notes

- Model nÃ y sá»­ dá»¥ng **Reverse Distillation** thay vÃ¬ propose noising nhÆ° tÃªn gá»i
- Training khÃ´ng cáº§n synthetic anomaly, chá»‰ train trÃªn normal images
- Test time sá»­ dá»¥ng cosine distance Ä‘á»ƒ tÃ­nh anomaly score
- Masking chá»‰ dÃ¹ng cho evaluation, khÃ´ng affect training process
- **WandB** giÃºp track experiments dá»… dÃ ng hÆ¡n so vá»›i TensorBoard
